
\section{Background}
    \textbf{Dec-POMDP.} We characterize a fully cooperative multiagent learning task as a \emph{decentralized partially observable Markov decision process} (Dec-POMDP) with a tuple $\mathcal{M} = \langle\mathcal{N}, \mathcal{S}, \mathcal{A}, T, \Omega, O, \Gamma, \gamma \rangle$ \cite{r1}. Here, $\mathcal{N} = \{1,2,\ldots, n\}$ is a set of agents, and $\mathcal{S}$ is a set consisting of environment states. At each time step, each agent $i \in \mathcal{N}$ chooses an action $a_i \in \mathcal{A}_i$, forming a joint action $a \in \mathcal{A} = \prod_{i=1}^n \mathcal{A}_i$ of all agents. This leads to a transition from the current state $s$ to the next state $s'$ governed by a transition function $T(s' \mid s, a)$. Due to the partial observability, $\Omega = \prod_{i=1}^n \Omega_i$ is the joint observation set, where $\Omega_i$ is the partial observation set of agent $i$. $O(o \mid s, a')$ is the conditional probability of joint observations given the current state $s$ and the previous joint action $a'$. $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ denotes the global reward function. The objective of the task is to maximize the total discounted reward $\sum_{t=0}^\infty \gamma^t r(s_t, a_t)$ where $\gamma \in [0, 1)$ is a discount factor, $s_t$ and $a_t$ are the state and the joint action at time step $t$, respectively.

    \textbf{Multiagent deep Q-learning.} For a given joint policy $\pi$ of agents, let the global action-value function $Q^\pi(s_t,a_t)=\mathbb{E}[\sum_{k=0}^\infty \gamma^i r(s_{t+k},a_{t+k})]$ denote the expected discounted reward starting at state $s_t$ with joint action $a_t$. When $\pi$ is optimal, the global action-value function satisfies the Bellman optimality equation. Due to the partial observation setting, we use the joint observation history in place of the state $s$. Multiagent deep Q-learning represents the global action-value function $Q_{\text{tot}}$ with a deep neural network parameterized by $\theta$. In centralized algorithms, the parameters are obtained by minimizing the expected TD error $\mathcal{L}= \mathbb{E} \left[ \left( r + \max_{a'} Q_{\text{tot}}(s', a'; \theta^-) - Q_{\text{tot}}(s, a; \theta) \right)^2 \right]$, where $\theta^-$ is the parameter of a target network.
    
    \textbf{Value-based methods in the CTDE paradigm.} A typical dilemma in cooperative multiagent learning is that each agent should act independently based on its observation, but global information is needed to make a good decision for teamwork. CTDE \cite{r2} is the popular framework in recent years to remedy this challenge. In most value-based methods in the CTDE paradigm, the individual action-value functions for all players are trained in a centralized way, and an agent only relies on its trained individual function and its partial observation to choose actions. Due to the partial observation, \cite{r3} suggests that the optimal joint strategy should be executed as each agent plays its own optimal strategy concurrently. Formally, the condition of such action-value functions can be described as follows:

    % In your document body:
    \begin{definition}[Individual-Global-Max (IGM) \cite{r4}]
        For a global action-value function $Q_{tot} : \mathcal{T} \times \mathcal{A} \to \mathbb{R}$ and a series of individual action-value functions $[Q_i]_{i=1}^n$ with $Q_i : \mathcal{T}_i \times \mathcal{A}_i \to \mathbb{R}$, where $\tau \in \mathcal{T}$ is the joint history and $\tau_i \in \mathcal{T}_i$ is agent $i$'s individual history, if $\forall \tau \in \mathcal{T}$
        \[
            \left(\arg\max_{a_1 \in \mathcal{A}_1} Q_1(\tau_1,a_1),\ldots, \arg\max_{a_n\in \mathcal{A}_n} Q_n(\tau_n,a_n)\right) \in \arg\max_{a \in \mathcal{A}} Q_{tot}(\tau,a),
        \]
        then we say that $[Q_i]_{i=1}^n$ satisfy the IGM condition for $Q_{tot}$.
    \end{definition}

    Once a framework satisfies the IGM condition, the global action value can be maximized efficiently via individually optimal choices of all the agents.

    \textbf{Self-attention mechanism.} 
    The self-attention mechanism \cite{r5} is widely used to extract the relationship between different positions of an input sequence in the natural language processing community. It can efficiently relate its different inputs. In the MARL community, the self-attention mechanism is used to learn the relationship between a team of agents. Jiang and Lu \cite{r6} employ the self-attention mechanism to learn the communication among agents. Li et al. \cite{r7} use the self-attention mechanism to determine the implicit coordination graph structure of agents. The formula of the self-attention mechanism can be written as:$
    \text{Attention}(\mathcal{Q}, \mathcal{K}, \mathcal{V}) = \text{softmax}(\mathcal{Q}\mathcal{K}^T)\mathcal{V},
    $where $\mathcal{Q}$, $\mathcal{K}$, and $\mathcal{V}$ denote the query vectors, the key vectors, and the value vectors, respectively. The weight of each value $\mathcal{V}_j$ to the $i$-th output is computed by the compatibility $\mathcal{Q}_i\mathcal{K}_j$, a dot product of the $i$-th query vector and $j$-th key vector. The softmax activation function translates the dot product into a measurement of attention.
    